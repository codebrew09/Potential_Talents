# -*- coding: utf-8 -*-
"""Project3 - HR Staffing - Conventional  Vectorization

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11zuEzG2r9mN8BrBg6yssjiAD6-Z6iUOZ
"""

from google.colab import drive
drive.mount('/content/drive')

dataset_path = '/content/drive/MyDrive/Machine Learning/potential-talents - Aspiring human resources - seeking human resources.csv'

#Importing necessary Libraries for pandas and visualization
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

#loading the dataset
## for this instance, we specified dataset_path variable assigned with our dataset
df = pd.read_csv(dataset_path)

"""# Exploratory Data Analysis (EDA)

## Univariate Analysis
"""

#print first 5 rows of the dataset
df.head(15)

df.describe()

"""The output only shows id and fit column as df.describe only includes columns with numerical values. Other columns have data type of "object"


For this dataset, df.describe() would be not useful as id columns only shows user numbering and not the value that gives importance for that data.
"""

#print the dataframe's shae in rows x column
df.shape

#printing dataframe's data types
df.dtypes

"""From the output of dtypes, fit is showing float64 even if it has empty values.

In Pandas, empty (or missing) values are represented as NaN (Not a Number). The NaN value is a special floating-point value.
"""

#printing the sum of the columns with no data.
#isnull() is the same with isna()
df.isnull().sum()

"""From the output above, fit is showing total of 104 with empty columns. This is expected as what we're trying to achieve for this project is to predict the fit column of either 0 or 1."""

#removing fit column from the dataset as it is the target variable
#this will help in ensuring that from analysis and ML model to be applied at a later stage, this variable will
#not affect the model's performance.
df.drop(columns=['fit'], inplace=True)

#revalidating if fit column has been removed
df.head()

#print unique valies from the "connection" column
df['connection'].unique()

#print the overall count of unique values found in "connection" column
df['connection'].nunique()

#print the count of how many times the unique values appeared in a column
df['connection'].value_counts()

df['job_title'].nunique()

df['location'].nunique()

# Bar plot for connections
df['connection'].value_counts().head(10).plot(kind='bar', title='Top 10 Connections')
plt.xlabel('Connection')
plt.ylabel('Count')
plt.show()

# Bar plot for Job Title
df['job_title'].value_counts().head(10).plot(kind='bar', title='Top 10 Job Title')
plt.xlabel('Job Title')
plt.ylabel('Count')
plt.show()

# Bar plot for locaiton
df['location'].value_counts().head(10).plot(kind='bar', title='Top 10 Job location')
plt.xlabel('Location')
plt.ylabel('Count')
plt.show()

"""At a glance, we notice the following from the dataset.

From the dataset:
- it has a total of 104 data (as seen in the row count)
- we have 5 columns namely : ID, job_title, location, connection, fit
- fit column is empty or NaN as this is the target variable we are predicting.
- there are 33 out of 104 unique values from the connection column
- there are 52 out of 104 unique values from the job_title column
- there are 41 out of 104 unique values from the location column

From Visualization:
- Majority of the users has 500+ connections (40+)
- Majority of location is in Kanada with a total of 12 users
- Job Title shows multiple jobs with same count where majority is in 4-7 instances.

## Biviariate Analysis
"""

crosstab = pd.crosstab(df['job_title'], df['location'])
sns.heatmap(crosstab, cmap="YlGnBu")
plt.title('Job Title vs Location')
plt.show()

crosstab = pd.crosstab(df['job_title'], df['connection'])
sns.heatmap(crosstab, cmap="YlGnBu")
plt.title('Job Title vs connection')
plt.show()

crosstab = pd.crosstab(df['location'], df['connection'])
sns.heatmap(crosstab, cmap="YlGnBu")
plt.title('Location vs connection')
plt.show()

"""#Creating Word Cloud"""

#installing wordcloud in jupyter notebook
!pip install wordcloud

#loading wordcloud library to jupyter notebook
from wordcloud import WordCloud

# Remove NaNs and convert to lowercase (optional)
job_titles_wc = df['job_title'].dropna().str.lower()

# Join all job titles into a single string
text = ' '.join(job_titles_wc)

# Generate and display a word cloud to visualize the most frequent job titles
wordcloud_wc = WordCloud(width=800, height=400, background_color='white').generate(text)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud_wc, interpolation='bilinear')
plt.axis('off')
plt.title('WordCloud of Job Titles')
plt.show()

# Get frequencies dictionary
frequencies = wordcloud_wc.words_

# Printing top 10 ranked words
print("Top 10 words ranked from Word Cloud - Term Frequency")
top_10 = sorted(frequencies.items(), key=lambda x: x[1], reverse=True)[:10]
for word, score in top_10:
  print(f"{word}: {score:.4f}")

"""Computation observed from the output are the term frequency (TF) where it is computed based on how many times earch word appears in the text.

#Applying TF-IDF Vectorization for the Job_Title
"""

#importing required library for TF-IDF Vectorization
from sklearn.feature_extraction.text import TfidfVectorizer

# Drop NaNs and lowercase the text
job_titles_tfidf = df['job_title'].dropna().str.lower().tolist()

#Creating TF-IDF Vectorizer
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(job_titles_tfidf)

# Mean TF-IDF score for each word
mean_tfidf = np.asarray(X.mean(axis=0)).flatten()
words = vectorizer.get_feature_names_out()

# Create dictionary of words and their average TF-IDF scores
tfidf_dict = dict(zip(words, mean_tfidf))

wordcloud_tfidf = WordCloud(width=800, height=400, background_color='white')
wordcloud_tfidf.generate_from_frequencies(tfidf_dict)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud_tfidf, interpolation='bilinear')
plt.axis('off')
plt.title('TF-IDF WordCloud of Job Titles')
plt.show()

sorted_tfidf = sorted(tfidf_dict.items(), key=lambda item: item[1], reverse=True)

print("Top 10 words by average TF-IDF score:")
for word, score in sorted_tfidf[:10]:
    print(f"{word}: {score:.4f}")

"""#Vectorizing Search Term"""

# Convert multiple search terms (like job titles or keywords) into vectors
# This allows us to compare the search terms to existing job titles numerically.
search_term = ["student"]
search_vector = vectorizer.transform(search_term)

from sklearn.metrics.pairwise import cosine_similarity

# Get similarity between search vector and all job titles
similarity_scores = cosine_similarity(search_vector, X)

for i, term in enumerate(search_term):
    print(f"\nðŸ” Top matches for search term: '{term}'")
    scores = similarity_scores[i]
    top_indices = np.argsort(scores)[::-1]  # descending sort

    for rank, idx in enumerate(top_indices[:10], start=1):  # Top 10 results
        title = job_titles_tfidf[idx]
        similarity = scores[idx]
        print(f"{rank}. {title} - Similarity: {similarity:.4f}")

job_titles_tfidf_df = df['job_title'].dropna().str.lower().reset_index()

for i, term in enumerate(search_term):
    print(f"\nðŸ” Top matches for search term: '{term}'")
    scores = similarity_scores[i]
    top_indices = np.argsort(scores)[::-1]

    #seen_titles = set()
    rank = 1

    for idx in top_indices:
        title = job_titles_tfidf_df.loc[idx, 'job_title']
        original_index = job_titles_tfidf_df.loc[idx, 'index']

        #if title not in seen_titles:
        print(f"{rank}. {title} (Index {original_index}) - Similarity: {scores[idx]:.4f}")
            #seen_titles.add(title)
        rank += 1

        if rank > 10:
            break

"""#Word2Vec Implementation"""

!pip install gensim

import gensim
from gensim.models import Word2Vec
import nltk
from nltk.tokenize import word_tokenize
import string

nltk.download('punkt_tab')

# Lowercase, remove punctuation, tokenize
job_titles_w2v = df['job_title'].dropna().str.lower()
job_titles_w2v = job_titles_w2v.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))
tokenized_titles = job_titles_w2v.apply(word_tokenize).tolist()

# Train Word2Vec on tokenized job titles
model = Word2Vec(sentences=tokenized_titles, vector_size=100, window=5, min_count=1, workers=4)

model.wv.most_similar("student")

def get_phrase_vector(phrase, model):
    tokens = word_tokenize(phrase.lower())
    tokens = [t for t in tokens if t in model.wv]
    if not tokens:
        return np.zeros(model.wv.vector_size)
    return np.mean([model.wv[t] for t in tokens], axis=0)

# Vectorize all job titles
job_title_vectors = np.array([get_phrase_vector(title, model) for title in job_titles_w2v])
#job_title_vectors = np.vstack([get_phrase_vector(title, model) for title in job_titles_w2v])

search_term = "student"
search_vector = get_phrase_vector(search_term, model).reshape(1, -1)

# Calculate cosine similarity
#similarities = cosine_similarity(search_vector, job_title_vectors).flatten()
similarities = cosine_similarity(search_vector, job_title_vectors).flatten()

# Sort and show top results
top_indices = np.argsort(similarities)[::-1]

print(f"\nðŸ” Top matches for search term: '{search_term}'")
for rank, idx in enumerate(top_indices[:10], start=1):
    print(f"{rank}. {job_titles_w2v.iloc[idx]} - Similarity: {similarities[idx]:.4f}")

search_term = "student"
search_vector = get_phrase_vector(search_term, model).reshape(1, -1)

# Calculate cosine similarity
similarities = cosine_similarity(search_vector, job_title_vectors).flatten()

# Sort and show top results
top_indices = np.argsort(similarities)[::-1]

print(f"\nðŸ” Top matches for search term: '{search_term}'")
for rank, idx in enumerate(top_indices[:10], start=1):
    applicant_id = df['id'].iloc[idx]
    job_title = job_titles_w2v.iloc[idx]
    similarity = similarities[idx]
    print(f"{rank}. ID: {applicant_id} - Job Title: {job_title} - Similarity: {similarity:.4f}")

"""#GloVe (Global Venctors for Word Representation) Implementation"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

glove_words = df['job_title'].dropna().str.lower()

# Create and fit the tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(glove_words)

print("Number of unique words in dictionary =", len(tokenizer.word_index))
print("Dictionary is =", tokenizer.word_index)

"""The dictionary output shows that after fitting the tokenizer, it gives each words index (1, 2, 3, and so on) where 1st index (in this case human) is the most frequent word from the dataset.

In a way, it is sorting the words based on the frequency it has been used from the dataset from the most frequent to least frequent based on index number
"""

def embedding_for_vocab(filepath, word_index, embedding_dim):
    vocab_size = len(word_index) + 1  # +1 for padding token (index 0)
    embedding_matrix_vocab = np.zeros((vocab_size, embedding_dim))

    with open(filepath, encoding="utf8") as f:
        for line in f:
            word, *vector = line.split()
            if word in word_index:
                idx = word_index[word]
                embedding_matrix_vocab[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]

    return embedding_matrix_vocab

# Download the GloVe dataset
!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip

#Unzip the file
!unzip -q glove.6B.zip

# Set embedding dimension (match this with glove file)
embedding_dim = 50

# Path to GloVe file
glove_path = './glove.6B.50d.txt'

# Generate embedding matrix
embedding_matrix_vocab = embedding_for_vocab(glove_path, tokenizer.word_index, embedding_dim)

# Print the dense vector for the first word in the tokenizer index
first_word_index = 1  # Tokenizer indexes start from 1
print("Dense vector for word with index 1 =>", embedding_matrix_vocab[first_word_index])

from numpy.linalg import norm

def cosine_similarity_glove(vec1, vec2):
    if norm(vec1) == 0 or norm(vec2) == 0:
        return 0.0
    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))

def sentence_to_avg_vector(sentence, tokenizer, embedding_matrix, embedding_dim):
    tokens = tokenizer.texts_to_sequences([sentence.lower()])[0]
    valid_vectors = [embedding_matrix[i] for i in tokens if i > 0 and i < embedding_matrix.shape[0]]
    if not valid_vectors:
        return np.zeros(embedding_dim)
    return np.mean(valid_vectors, axis=0)

search_term = "student"
embedding_dim = 50  # same as you used for GloVe
search_vector = sentence_to_avg_vector(search_term, tokenizer, embedding_matrix_vocab, embedding_dim)

# Store titles and similarity scores
similarities = []

for title in glove_words:
    title_vector = sentence_to_avg_vector(title, tokenizer, embedding_matrix_vocab, embedding_dim)
    similarity_score = cosine_similarity_glove(search_vector, title_vector)
    similarities.append((title, similarity_score))

# Sort results
similarities.sort(key=lambda x: x[1], reverse=True)

# Display top 10 matches
print(f"\nðŸ” Top matches for search term: '{search_term}'")
for rank, (title, score) in enumerate(similarities[:10], 1):
    print(f"{rank}. {title} - Similarity: {score:.4f}")

search_term = "student"
embedding_dim = 50  # same as used for GloVe
search_vector = sentence_to_avg_vector(search_term, tokenizer, embedding_matrix_vocab, embedding_dim)

# Store (id, title, similarity) tuples
similarities = []

# Use df.dropna to avoid rows with missing job titles
for _, row in df.dropna(subset=['job_title']).iterrows():
    title = row['job_title'].lower()
    applicant_id = row['id']  # This is your unique applicant index

    title_vector = sentence_to_avg_vector(title, tokenizer, embedding_matrix_vocab, embedding_dim)
    similarity_score = cosine_similarity_glove(search_vector, title_vector)

    similarities.append((applicant_id, title, similarity_score))

# Sort results
similarities.sort(key=lambda x: x[2], reverse=True)

# Display top 10 matches
print(f"\nðŸ” Top matches for search term: '{search_term}'")
for rank, (applicant_id, title, score) in enumerate(similarities[:10], 1):
    print(f"{rank}. {title} (Applicant ID: {applicant_id}) - Similarity: {score:.4f}")

"""#fasttext by facebook Implementation"""

!pip install fasttext

#from gensim.models import FastText
import nltk
from nltk.tokenize import word_tokenize
import string
import fasttext

# Download NLTK tokenizer data
# nltk.download('punkt_tab')

# Clean and lowercase job titles
job_titles_ft = df['job_title'].dropna().str.lower()

# Save each job title as one line in job_titles.txt
with open("job_titles.txt", "w", encoding="utf-8") as f:
    for title in job_titles_ft:
        f.write(title + "\n")

model_ft = fasttext.train_unsupervised(
    'job_titles.txt',
    model='skipgram',   # or 'cbow'
    dim=100,            # vector size
    ws=5,               # context window
    minCount=1          # keep all words (like min_count in gensim)
)

# Save
model_ft.save_model("job_titles_ft.bin")

# Load later
model_ft = fasttext.load_model("job_titles_ft.bin")

vec_ft = model_ft.get_word_vector("student")

# Get vectors for a sentence (average of word vectors)
def get_phrase_vector_ft(phrase, model):
    words = phrase.lower().split()
    vectors = [model.get_word_vector(w) for w in words]
    return sum(vectors) / len(vectors) if vectors else np.zeros(model.get_dimension())

# Load job titles from file
with open("job_titles.txt", "r", encoding="utf-8") as f:
    job_titles_ft = [line.strip() for line in f]

# Create embeddings for all job titles
job_title_vectors_ft = np.array([get_phrase_vector_ft(title, model_ft) for title in job_titles_ft])

search_term = "student"
search_vector_ft = get_phrase_vector_ft(search_term, model_ft).reshape(1, -1)

# Compute cosine similarities
similarities = cosine_similarity(search_vector_ft, job_title_vectors_ft).flatten()

# Sort results
top_indices = np.argsort(similarities)[::-1]

print(f"\nðŸ” Top matches for '{search_term}':")
for i in top_indices[:10]:
    print(f"{job_titles_ft[i]} - Similarity: {similarities[i]:.4f}")

"""# FastText by gensim Implementation"""

from gensim.models import FastText
import nltk
from nltk.tokenize import word_tokenize
import string

# Download NLTK tokenizer data
nltk.download('punkt_tab')

# Lowercase, remove punctuation, tokenize
job_titles_FT = df['job_title'].dropna().str.lower()
job_titles_FT = job_titles_FT.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))
tokenized_titles_FT = job_titles_FT.apply(word_tokenize).tolist()

#Train FastText Model

model_FT = FastText(
    sentences=tokenized_titles_FT,
    vector_size=100,  # dimensionality of vectors
    window=5,         # context window size
    min_count=1,      # include words that appear at least once
    workers=4,        # number of threads
    sg=1              # skip-gram model; set to 0 for CBOW
)

# Helper Function
def get_phrase_vector_FT(phrase, model):
    """Convert phrase to vector by averaging token vectors."""
    tokens = word_tokenize(phrase.lower())
    tokens = [t for t in tokens if t in model.wv]  # filter out OOV tokens
    if not tokens:
        return np.zeros(model.vector_size)
    return np.mean([model.wv[t] for t in tokens], axis=0)

# Vectorize All Job Titles
job_title_vectors_FT = np.array([get_phrase_vector_FT(title, model_FT) for title in job_titles_FT])

# Search & Display Results

search_term_FT = "student"
search_vector_FT = get_phrase_vector_FT(search_term_FT, model_FT).reshape(1, -1)

# Calculate cosine similarity
similarities = cosine_similarity(search_vector_FT, job_title_vectors_FT).flatten()

# Sort by highest similarity
top_indices = np.argsort(similarities)[::-1]

print(f"\nðŸ” Top matches for search term: '{search_term_FT}'")
for rank, idx in enumerate(top_indices[:10], start=1):
    applicant_id = df['id'].iloc[idx]
    job_title_FT = job_titles_FT.iloc[idx]
    similarity = similarities[idx]
    print(f"{rank}. ID: {str(applicant_id)} - Job Title: {job_title_FT} - Similarity: {similarity:.4f}")

"""#Contextualized Embedding

##Sentence-Transformers
"""

!pip install sentence-transformers

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Load a pre-trained model (good trade-off: fast + accurate)
model_ctx = SentenceTransformer('all-MiniLM-L6-v2')

# Encode job titles
job_titles_ctx = df['job_title'].dropna().tolist()
job_title_vectors_ctx = model_ctx.encode(job_titles_ctx, convert_to_numpy=True)

# Encode search term
search_term_ctx = "student"
search_vector_ctx = model_ctx.encode([search_term_ctx], convert_to_numpy=True)

# Calculate similarity
similarities = cosine_similarity(search_vector_ctx, job_title_vectors_ctx).flatten()

# Sort results
top_indices = np.argsort(similarities)[::-1]

print(f"\nðŸ” Top contextual matches for search term: '{search_term_ctx}'")
for rank, idx in enumerate(top_indices[:10], start=1):
    applicant_id = df['id'].iloc[idx]
    job_title = job_titles_ctx[idx]
    similarity = similarities[idx]
    print(f"{rank}. ID: {applicant_id} - Job Title: {job_title} - Similarity: {similarity:.4f}")

"""## HuggingFace Transformers"""

!pip install transformers torch

from transformers import AutoTokenizer, AutoModel
import torch

# Load BERT model + tokenizer
tokenizer_HF = AutoTokenizer.from_pretrained("bert-base-uncased")
model_HF = AutoModel.from_pretrained("bert-base-uncased")

def get_embedding_HF(text):
    inputs = tokenizer_HF(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model_HF(**inputs)
    # CLS token embedding (alternative: mean pooling across tokens)
    return outputs.last_hidden_state[:, 0, :].numpy()

# ----- Encode your dataframe job titles -----
job_titles_HF = df['job_title'].dropna().tolist()
job_title_vectors_HF = np.vstack([get_embedding_HF(title) for title in job_titles_HF])

# ----- Encode search term -----
search_term_HF = "student"
search_vector_HF = get_embedding_HF(search_term_HF)

# ----- Compute similarity -----
similarities = cosine_similarity(search_vector_HF, job_title_vectors_HF).flatten()

# ----- Sort results -----
top_indices = np.argsort(similarities)[::-1]

print(f"\nðŸ” Top contextual matches for search term: '{search_term_ctx}'")
for rank, idx in enumerate(top_indices[:10], start=1):
    applicant_id = df['id'].iloc[idx]
    job_title = job_titles_ctx[idx]
    similarity = similarities[idx]
    print(f"{rank}. ID: {applicant_id} - Job Title: {job_title} - Similarity: {similarity:.4f}")

"""#Vanilla BERT implementation"""

#Installing BERT dependencies
!pip install torch transformers scikit-learn

#Loading Pre-trained BERT model
import torch
from transformers import BertTokenizer, BertModel

# Load pretrained BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

#Encoding search term for BERT model

def encode_text_bert(text, tokenizer, model):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)

    # Get hidden states from last layer
    last_hidden_state = outputs.last_hidden_state  # shape: [batch, tokens, hidden]

    # Mean pooling to get sentence vector
    sentence_embedding = last_hidden_state.mean(dim=1)  # shape: [batch, hidden]
    return sentence_embedding.squeeze().numpy()

#Encoding search term and Job Titles
search_term_bert = "student"
job_titles_bert = ["aspiring human resources", "university", "manager", "teacher"]

# Encode search term
search_vector_bert = encode_text_bert(search_term_bert, tokenizer, model)

# Encode job titles
job_title_vectors_bert = [encode_text_bert(title, tokenizer, model) for title in job_titles_bert]

#Cosine Similarity for BERT implementation
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

similarities = cosine_similarity([search_vector_bert], job_title_vectors_bert).flatten()

# Rank by similarity
top_indices = np.argsort(similarities)[::-1]

print(f"\nðŸ” Top matches for '{search_term}':")
for idx in top_indices:
    print(f"{job_titles_bert[idx]} -> {similarities[idx]:.4f}")

"""## Project Conclusion

This project presents a semantic job-title matching system designed to improve how candidate profiles are searched and ranked based on relevance to a given role or keyword. The objective was to move beyond simple keyword matching and build an intelligent system that understands the meaning of job titles, enabling smarter talent discovery.

We explored a progression of NLP techniques:

- TF-IDF Based Similarity â€“ Served as the foundational baseline. It captures term importance but struggles with synonyms and contextual meaning.
Word Embedding Models (Word2Vec, GloVe, FastText) â€“ Introduced semantic understanding by representing words as dense vectors, allowing similarity comparisons based on meaning rather than exact words.
Contextual Embeddings (Sentence-Transformers, BERT) â€“ Leveraged transformer models to capture deeper contextual relationships, producing highly accurate semantic similarity between search terms and job titles.

### ðŸ”‘ Key Insights

Traditional vectorization (TF-IDF) works for surface-level matching but lacks semantic depth.

Word embeddings (Word2Vec, GloVe, FastText) significantly improve similarity search by capturing relationships between words.

Subword modeling (FastText) handles rare or unseen terms better than older embedding methods.

Contextual models (Sentence-BERT, BERT) deliver the most accurate relevance ranking because they understand language in context, not just word frequency.

Semantic similarity can transform recruitment search from keyword-based filtering to meaning-based candidate discovery.

### ðŸš€ Final Takeaway

This project demonstrates how progressively advanced NLP techniques â€” from TF-IDF to transformer-based embeddings â€” can be applied to build an intelligent job-title matching and candidate search system. By representing job roles and search queries as semantic vectors, we enable scalable, accurate, and context-aware talent retrieval.

Such systems form the foundation of modern AI-powered recruitment platforms, where search, ranking, and recommendation engines rely on deep language understanding rather than manual filtering or rigid keyword rules.
"""

