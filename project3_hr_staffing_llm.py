# -*- coding: utf-8 -*-
"""Project3 - HR Staffing - LLM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bafUiHKMbGfW_Wb0mHkacIKO_LEY48pN

# Potential Talents Ranking System  
### LLM-Based Candidate Fitness Prediction & Intelligent Re-Ranking

## Problem Overview

Talent sourcing teams spend significant manual effort identifying high-potential candidates for specific job roles. The challenge is not only finding candidates, but ranking them based on how well they match a role.

In this project, we design an **AI-powered ranking system** that:

- Predicts how fit a candidate is for a job role
- Ranks candidates automatically
- Learns from recruiter feedback
- Reduces manual screening time

We experiment with three approaches:

1. **Prompt-based ranking (No fine-tuning)**
2. **Fine-tuned LLM scoring**
3. **RAG (Retrieval-Augmented Ranking)**

## Environment Setup

In this section, we prepare the environment for working with large language models.

Key actions:
- Mount Google Drive to access data
- Install memory-efficient libraries (bitsandbytes)
- Log into Hugging Face to access the LLM

This allows us to load and fine-tune a large language model using limited computational resources.
"""

# Mounting google drive to google collab

from google.colab import drive
drive.mount('/content/drive')

# Hugging Face account login and authentication for LLM models access

from huggingface_hub import login
login("<redacted>")

!pip install -q -U bitsandbytes
!pip install -q -U accelerate
!pip install peft
!pip install -U trl
!pip install sentence-transformers faiss-cpu

# Importing and loading required libraries

import torch
import re
import transformers
import trl
import numpy as np
import faiss
import pandas as pd

from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig, TrainingArguments
from peft import prepare_model_for_kbit_training,LoraConfig,PeftModel,get_peft_model,LoraConfig,prepare_model_for_kbit_training
from datasets import load_dataset, Dataset
from trl import SFTTrainer
from sentence_transformers import SentenceTransformer
from datetime import datetime

"""##  Data Loading and Preparation

We load the candidate dataset containing structured information such as skills, experience, and descriptions.

Steps performed:
- Read dataset into a DataFrame
- Remove the original "fit" column (we want the model to predict fitness instead)
- Convert structured candidate information into natural language text
"""

# Loading the data

file_path = '/content/drive/MyDrive/Machine Learning/potential-talents - Aspiring human resources - seeking human resources.csv'
df = pd.read_csv(file_path)

df.head()

df = df.drop(columns=["fit"])

# Define Base LLM Model and Device

model_path = "meta-llama/Llama-3.2-3B-Instruct"
device = "cuda" # the device to load the model onto

# Quantization configuration

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=False,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float32
)

# Loading the model and tokenizer

model = AutoModelForCausalLM.from_pretrained(model_path,quantization_config=bnb_config)
tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    model_max_length=512,
    padding_side="left",
    add_eos_token=True)
tokenizer.pad_token = tokenizer.eos_token #need to study this parameter

df.head()

"""## Baseline Approach: Ranking Without Fine-Tuning

> Here we use the LLM **as-is**, without any training.

> We describe candidates in text and ask the LLM to act like a recruiter and rank them for a job role using prompt engineering.

> This simulates a recruiter giving instructions such as:
"Rank the following candidates for an Human Resource Role"

####**System Flow: Prompt-Based Ranking (No Fine-Tuning)**<br><br>

<center>Job Role (Search Term)
        <center>â†“<br>
Convert candidate data â†’ natural language profiles<
        <center>â†“<br>
Insert profiles into ranking prompt<br>
        <center>â†“<br>
Send prompt to base LLM
        <center>â†“<br>
LLM reasons using general knowledge
        <center>â†“<br>
LLM outputs ranked list of candidates<br><br>
"""

# Converts a structured profile row into a compact text description for prompting/embedding.

def profile_to_text(row):
    return f"""Job Title: {row['job_title']}
Location: {row['location']}
Connections: {row['connection']}"""

# Apply Text Conversion to Entire Dataset

df["profile_text"] = df.apply(profile_to_text, axis=1)

df.head()

# Builds an LLM prompt that asks the model to rank multiple candidate profiles for a search term.

def ranking_prompt(search_term, profiles):
    candidates = "\n".join(
        [f"{i+1}. {p}" for i, p in enumerate(profiles)]
    )

    return f"""You are a recruiter.

Search Requirement:
{search_term}

Candidates:
{candidates}

Task:
Rank the candidates from best to worst match.
Assign a relevance score from 1 (poor) to 5 (excellent).
Provide a brief explanation for each ranking.
"""

#Select Sample Candidate Profiles

profiles = df["profile_text"].tolist()[:5]

# Test Baseline Ranking

prompt = ranking_prompt(
    "Human Resource",
    profiles
)

outputs = model.generate(
    **tokenizer(prompt, return_tensors="pt").to(model.device),
    max_new_tokens=300
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# Test Baseline Ranking

prompt = ranking_prompt(
    "Student",
    profiles
)

outputs = model.generate(
    **tokenizer(prompt, return_tensors="pt").to(model.device),
    max_new_tokens=300
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# Test Baseline Ranking

prompt = ranking_prompt(
    "Engineer",
    profiles
)

outputs = model.generate(
    **tokenizer(prompt, return_tensors="pt").to(model.device),
    max_new_tokens=300
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))

"""## Advanced Approach: Fine-Tuning the LLM

> In this section, we improve the model by teaching it what makes a strong candidate.

> Instead of just prompting, we fine-tune the LLM using **QLoRA**, a memory-efficient method that allows training large models with limited hardware.

### Model Training Pipeline

####**System Flow: Fine-Tuned Ranking System - Model Training**<br><br>

<center>Raw Candidate Data
        <center>â†“<br>
Weak labeling (keyword-based fitness scores)
        <center>â†“<br>
Instruction-format training samples
        <center>â†“<br>
QLoRA Fine-Tuning
        <center>â†“<br>
Fine-Tuned Model<br><br>
"""

#Load Quantized Model for Efficient Training

MODEL_NAME = "meta-llama/Llama-3.2-3B-Instruct"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    dtype=torch.float16
)

base_model = prepare_model_for_kbit_training(base_model)

# Configure LoRA for Parameter-Efficient Fine-Tuning

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(base_model, peft_config)
model.print_trainable_parameters()

# Reload Dataset for Training Pipeline

file_path = '/content/drive/MyDrive/Machine Learning/potential-talents - Aspiring human resources - seeking human resources.csv'
df_candidates = pd.read_csv(file_path)

df_candidates["profile_text"] = (
    "Job Title: " + df_candidates["job_title"].astype(str) + ", " +
    "Location: " + df_candidates["location"].astype(str) + ", " +
    "Connections: " + df_candidates["connection"].astype(str)
)

# Define Search Terms & Expand Training Data

search_terms = ["HR", "Human Resource", "Student", "Aspiring", "Engineer"]

expanded_rows = []

for _, row in df_candidates.iterrows():
    for term in search_terms:
        expanded_rows.append({
            "search_term": term,
            "profile_text": row["profile_text"]
        })

df_train = pd.DataFrame(expanded_rows)

# Generates a simple heuristic relevance score based on keyword overlap between search term and profile.

def weak_label(search_term, profile):
    st = search_term.lower()
    pf = profile.lower()

    if any(word in pf for word in st.split()):
        return 5
    return 2

# Formats a supervised fine-tuning training example using weak labels for instruction-style learning.

def build_training_sample(row):
    score = weak_label(row["search_term"], row["profile_text"])

    return f"""
### Instruction:
You are an AI recruitment assistant that ranks candidate relevance.

### Search Term:
{row['search_term']}

### Candidate Profile:
{row['profile_text']}

### Task:
Rate relevance from 1 (poor match) to 5 (strong match) and explain briefly.

### Response:
Score: {score}
Reason: The candidate profile was compared with the search term using keyword alignment and role similarity.
"""

# Prepare Dataset for Model Fine-Tuning

df_train["text"] = df_train.apply(build_training_sample, axis=1)

dataset = Dataset.from_pandas(df_train[["text"]])
dataset = dataset.train_test_split(test_size=0.1, seed=42)

# Define Training Hyperparameters

training_args = TrainingArguments(
    output_dir="./qlora_results",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=3,
    fp16=False,
    bf16=True,  # Explicitly set to True
    logging_steps=10,
    optim="adamw_torch",
    save_strategy="epoch",
    eval_strategy="epoch",
    report_to="none"
)

# Initialize Fine-Tuning Trainer

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    processing_class=tokenizer,
    args=training_args,
)

trainer.model.print_trainable_parameters()

# Start Model Fine-Tuning

trainer.train()

# Save Fine-Tuned Adapter and Tokenizer

trainer.model.save_pretrained("./qlora_adapter")
tokenizer.save_pretrained("./qlora_adapter")

# Creates the inference-time prompt for scoring a single candidate against a search term.

def build_inference_prompt(search_term, profile):
    return f"""
### Instruction:
You are an AI recruitment assistant that ranks candidate relevance.

### Search Term:
{search_term}

### Candidate Profile:
{profile}

### Task:
Rate relevance from 1 to 5 and explain briefly.

### Response:
"""

# Runs the LLM to generate a relevance evaluation for one (search term, profile) pair.

def get_score(term, profile):
    prompt = build_inference_prompt(term, profile)

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    outputs = model.generate(
        **inputs,
        max_new_tokens=80,
        temperature=0.2
    )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

"""### Application (Inference) Pipeline

####**System Flow: Fine-Tuned Ranking System - Application Pipeline**<br><br>
<center>New Job Role + Candidate Profiles<br>
        <center>â†“<br>
Convert profiles to text
        <center>â†“<br>
Send to Fine-Tuned Model
        <center>â†“<br>
Model predicts fitness score
        <center>â†“<br>
Sort by score
        <center>â†“<br>
Ranked Candidate List <br><br>
"""

# Reload Fine-Tuned Model for Inference

MODEL_NAME = "meta-llama/Llama-3.2-3B-Instruct"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16
)

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained("./qlora_adapter")

# Load base model again
base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# ðŸ”¥ Attach LoRA weights
model = PeftModel.from_pretrained(base_model, "./qlora_adapter")
model.eval()

# Constructs the scoring prompt template used before sending input to the language model.

def build_prompt(search_term, profile_text):
    return f"""
### Instruction:
You are an AI recruitment assistant that ranks candidate relevance.

### Search Term:
{search_term}

### Candidate Profile:
{profile_text}

### Task:
Rate relevance from 1 to 5 and explain briefly.

### Response:
"""

# Uses the LLM to score a candidate profile and returns both numeric score and model explanation.

def get_candidate_score(search_term, profile_text):
    prompt = build_prompt(search_term, profile_text)

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.1
        )

    text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extract score
    match = re.search(r"Score:\s*(\d)", text)
    score = int(match.group(1)) if match else 0

    return score, text

# Scores all candidate profiles in a dataframe and returns them sorted by relevance.

def rank_candidates(search_term, df):
    results = []

    for _, row in df.iterrows():
        score, response = get_candidate_score(search_term, row["profile_text"])

        results.append({
            "profile": row["profile_text"],
            "score": score,
            #"llm_output": response
        })

    ranked = sorted(results, key=lambda x: x["score"], reverse=True)
    return ranked

# Run Ranking

search_term = "Human Resource"

ranked_candidates = rank_candidates(search_term, df)

for r in ranked_candidates[:5]:
    print("Score:", r["score"])
    print(r["profile"])
    print()

# Run Ranking

search_term = "Student"

ranked_candidates = rank_candidates(search_term, df)

for r in ranked_candidates[:5]:
    print("Score:", r["score"])
    print(r["profile"])
    print()

# Run Ranking

search_term = "engineer"

ranked_candidates = rank_candidates(search_term, df)

for r in ranked_candidates[:5]:
    print("Score:", r["score"])
    print(r["profile"])
    print()

"""## Retrieval-Augmented Ranking (RAG)

>Instead of sending all candidates to the model, we first retrieve the most relevant ones using semantic search.

>Large candidate pools can overwhelm LLMs. Retrieval narrows down the best matches before scoring.

###**System Flow: RAG-Based Ranking**<br><br>

<center>All Candidate Profiles<br>
        <center>â†“<br>
Convert profiles to embeddings (vector representation)
        <center>â†“<br>
Store embeddings for search<br><br>

<center>Job Role Query
        <center>â†“<br>
Convert role to embedding
        <center>â†“<br>
Similarity search (retrieve top-K relevant candidates)
        <center>â†“<br>
Selected candidate subset
        <center>â†“<br>
Send subset to Fine-Tuned LLM
        <center>â†“<br>
Model predicts fitness scores
        <center>â†“<br>
Final ranked list<br><br>
"""

# Load Sentence Embedding Model

embed_model = SentenceTransformer("all-MiniLM-L6-v2")  # fast + good

# Generate Embeddings for All Profiles

profiles = df["profile_text"].tolist()

embeddings = embed_model.encode(
    profiles,
    convert_to_numpy=True,
    show_progress_bar=True
)

# Build FAISS Vector Index

dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

# Retrieves top-k semantically similar candidate profiles using vector embeddings and FAISS search.

def retrieve_candidates(search_term, top_k=5):
    query_vec = embed_model.encode([search_term])
    distances, indices = index.search(query_vec, top_k)

    return df.iloc[indices[0]]

# Performs RAG-style ranking: retrieve top candidates via embeddings, then re-rank using the LLM.

def rag_rank(search_term, top_k=5):
    retrieved_df = retrieve_candidates(search_term, top_k)
    results = []

    for _, row in retrieved_df.iterrows():
        score, response = get_candidate_score(search_term, row["profile_text"])

        results.append({
            "profile": row["profile_text"],
            "score": score,
            "llm_output": response
        })

    return sorted(results, key=lambda x: x["score"], reverse=True)

# Run RAG Ranking - Query

search_term = "Human Resource"

ranked = rag_rank(search_term, top_k=10)

for r in ranked:
    print("Score:", r["score"])
    print(r["profile"])
    print()

# Run RAG Ranking - Query

search_term = "Student"

ranked = rag_rank(search_term, top_k=10)

for r in ranked:
    print("Score:", r["score"])
    print(r["profile"])
    print()

# Run RAG Ranking - Query

search_term = "Engineer"

ranked = rag_rank(search_term, top_k=10)

for r in ranked:
    print("Score:", r["score"])
    print(r["profile"])
    print()

"""## Summary of Approaches

| Approach | Learning | Strength | Weakness |
|----------|----------|----------|----------|
| Prompt Only | No | Fast | Inconsistent |
| Fine-Tuned LLM | Yes | Accurate | Requires training |
| RAG + LLM | Yes | Scalable | More complex |

<br>This project demonstrates how modern AI systems combine:<br>
LLMs + Retrieval + Human Feedback
to solve real-world ranking problems.

## Project Conclusion

This project presents an AI-powered system for ranking job candidates based on role fitness. The goal was to reduce manual screening effort and create a scalable, intelligent candidate evaluation pipeline.

We explored three approaches:

- **Prompt-Based Ranking** â€“ Used an LLM without training as a baseline. Fast but inconsistent.
- **Fine-Tuned Ranking** â€“ Trained the model using weak supervision and QLoRA, enabling more reliable and role-aware fitness scoring.
- **RAG-Based Ranking** â€“ Added semantic retrieval to narrow candidates before scoring, improving relevance and scalability.

### ðŸ”‘ Key Insights
- LLMs can function as evaluators, not just text generators.
- Fine-tuning improves consistency and decision quality.
- Retrieval mechanisms make large-scale ranking feasible.
- Human feedback can be integrated to continuously improve rankings.

### ðŸš€ Final Takeaway
This project demonstrates how combining **LLMs, fine-tuning, retrieval systems, and human-in-the-loop learning** can build a practical AI solution for modern talent sourcing and candidate ranking.
"""

